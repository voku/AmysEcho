# Amyâ€™s Echo

**This repository contains the development work for my little girl â€” to help her be understood, to help her learn, and to help others understand her world.**

Amy is four years old. She was born with **22q11 Deletion Syndrome** and communicates using **German Sign Language (DGS)**. Her gestures are expressive, her intent is clear â€” but most people around her donâ€™t understand what sheâ€™s trying to say.

This project aims to fix that.

---

## ğŸ¯ Purpose

> Donâ€™t build for everyone. Build for one. But do it well enough that everyone could follow.

Amyâ€™s Echo is a gesture recognition system designed to translate DGS into speech and symbols â€” in real time, offline if needed, and always with clarity and care.

This is not a demo or experiment. Itâ€™s a production-grade, full-stack project with one goal:

> **Turn Amyâ€™s gesture into understanding. Every time.**

---

## ğŸ§± Tech Stack? 

| Layer             | Tech                          | Purpose                                           |
|------------------|-------------------------------|---------------------------------------------------|
| App Framework     | React Native (CLI)            | Cross-platform + native module access             |
| Language          | TypeScript (strict mode)      | Predictable, type-safe code                       |
| Camera            | `react-native-vision-camera`  | High-performance gesture capture                  |
| ML Inference      | `react-native-fast-tflite`    | Local fallback via TensorFlow Lite                |
| Cloud ML          | Custom API                    | Accurate gesture classification                   |
| UI/UX             | RN Animated API + Skia (opt.) | Gentle, trust-based feedback                      |
| Audio             | `expo-av`, `expo-speech`      | Speech output + sound effects                     |
| Database          | WatermelonDB (SQLite)         | Encrypted, offline-first local storage            |

---

## ğŸ§  Architecture: Hybrid-First

Amyâ€™s Echo is designed around a hybrid loop:

1. **See**: Capture gesture via camera.
2. **Think**: Run ML classification (cloud preferred, local fallback).
3. **Speak + Show**: Output voice and symbol.
4. **Confirm**: Gentle haptic + visual confirmation.
5. **Learn**: Corrections are logged, models adapt over time.

Fallbacks are not optional. The system must **always** respond â€” even when uncertain.

---

## ğŸ”µ Interaction Flows (HIPs)

| Protocol | Purpose                                  |
|----------|------------------------------------------|
| HIP 1    | Onboarding (consent, first-use setup)    |
| HIP 2    | Teach mode (caregiver trains a new sign) |
| HIP 3    | Correction mode (â€œHelp Meâ€ repair flow)  |
| HIP 4    | Maintenance (â€œLetâ€™s practice this againâ€)|

---

## ğŸ—ƒï¸ Core Goals

- **Turn gestures into speech and visuals**
- **Work offline-first, no cloud dependency**
- **Handle uncertainty with grace, not silence**
- **Log every correction to learn and adapt**
- **Make it simple for a child to succeed**

---

## ğŸš§ Current Status

- [x] Spec ([markdown](./spec/AmysEcho.md))
- [x] React Native baseline setup
- [x] Camera + ML integration (initial hybrid recognizer)
- [x] HIP 1 + HIP 3 MVP implementation
- [x] HIP 2 training flow stub

## ğŸ›£ï¸ Roadmap

The next stages follow the broad implementation plan in the project spec.

- **Phase 1 â€“ Solidify the Foundation**
  - [ ] Integrate placeholder symbol images, videos and a basic gestures model
  - [ ] Finish `mlService` and `audioService` implementations
  - [ ] Expand profile management and vocabulary set selection
  - [ ] Build the app with the custom dev client on a device
- **Phase 2 â€“ Enhance Core Functionality**
  - [ ] Complete the gesture training workflow and store samples
  - [ ] Surface adaptive suggestions from the dialog engine
  - [ ] Add caregiver override actions in the CorrectionPanel
- **Phase 3 â€“ Refine and Polish**
  - [ ] Improve UI layout and feedback animations
  - [ ] Implement accessibility options like larger fonts and high contrast
  - [ ] Support optional DGS video playback for symbols
- **Phase 4 â€“ Advanced Features & Deployment**
  - [ ] Connect to an LLM for dynamic suggestions with privacy controls
  - [ ] Explore offline model retraining from collected data
  - [ ] Build the caregiver analytics dashboard
  - [ ] Prepare production builds for app store release

## â–¶ï¸ Running the mobile app

The React Native code lives in `app/`. Install dependencies with `npm install` inside that folder, then run `npm run ios` or `npm run android` to start a simulator. This skeleton includes onboarding, recognition, correction and training screens. Camera and ML integration now have an initial hybrid recognizer stub.

---

## ğŸ¤ Contributing

This is a focused project with one user. That means:

- âœ… Clean code, tested assumptions
- âœ… No â€œmove fastâ€ hacks
- âœ… Emotional context matters â€” build with care

If youâ€™re here to help: thank you.  
PRs are welcome, but **read the spec first**.

---

## ğŸ“„ License

MIT â€“ But with one request:  
**If you use this work to help another child â€” let me know.** Thatâ€™s why itâ€™s public.

---

## â¤ï¸ Built For

**Amy.**  
To help her be understood.  
To help her learn.  
To help the world finally listen.
