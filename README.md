# Amy‚Äôs Echo

**This repository contains the development work for my little girl ‚Äî to help her be understood, to help her learn, and to help others understand her world.**

Amy is four years old. She was born with **22q11 Deletion Syndrome** and communicates using **German Sign Language (DGS)**. Her gestures are expressive, her intent is clear ‚Äî but most people around her don‚Äôt understand what she‚Äôs trying to say.

This project aims to fix that.

---

## üéØ Purpose

> Don‚Äôt build for everyone. Build for one. But do it well enough that everyone could follow.

Amy‚Äôs Echo is a gesture recognition system designed to translate DGS into speech and symbols ‚Äî in real time, offline if needed, and always with clarity and care.

This is not a demo or experiment. It‚Äôs a production-grade, full-stack project with one goal:

> **Turn Amy‚Äôs gesture into understanding. Every time.**

---

## üß± Tech Stack? 

| Layer             | Tech                          | Purpose                                           |
|------------------|-------------------------------|---------------------------------------------------|
| App Framework     | React Native (CLI)            | Cross-platform + native module access             |
| Language          | TypeScript (strict mode)      | Predictable, type-safe code                       |
| Camera            | `react-native-vision-camera`  | High-performance gesture capture                  |
| ML Inference      | `react-native-fast-tflite`    | Local fallback via TensorFlow Lite                |
| Cloud ML          | Custom API                    | Accurate gesture classification                   |
| UI/UX             | RN Animated API + Skia (opt.) | Gentle, trust-based feedback                      |
| Audio             | `expo-av`, `expo-speech`      | Speech output + sound effects                     |
| Database          | WatermelonDB (SQLite)         | Encrypted, offline-first local storage            |

---

## ‚ö° Quick Setup

1. `npm install` ‚Äì install root dependencies
2. `cd app` ‚Äì open the app code
3. `npm install` - install mobile app deps
4. `npm test` ‚Äì run the Node test suite
   - Tests live in `app/test/` and cover both server and app modules.
5. `cd ../server && npm install && npm test` ‚Äì run Python training tests
   - These tests live in `server/test/` and require Python with `numpy` and `pytest`.
6. Inside `app`, run `npm run ios` or `npm run android` to launch the app

---

## üß† Architecture: Hybrid-First

Amy‚Äôs Echo is designed around a hybrid loop:

1. **See**: Capture gesture via camera.
2. **Think**: Run ML classification (cloud preferred, local fallback).
3. **Speak + Show**: Output voice and symbol.
4. **Confirm**: Gentle haptic + visual confirmation.
5. **Learn**: Corrections are logged, models adapt over time.

Fallbacks are not optional. The system must **always** respond ‚Äî even when uncertain.

---

## üîµ Interaction Flows (HIPs)

| Protocol | Purpose                                  |
|----------|------------------------------------------|
| HIP 1    | Onboarding (consent, first-use setup)    |
| HIP 2    | Teach mode (caregiver trains a new sign) |
| HIP 3    | Correction mode (‚ÄúHelp Me‚Äù repair flow)  |
| HIP 4    | Maintenance (‚ÄúLet‚Äôs practice this again‚Äù)|

---

## üóÉÔ∏è Core Goals

- **Turn gestures into speech and visuals**
- **Work offline-first, no cloud dependency**
- **Handle uncertainty with grace, not silence**
- **Log every correction to learn and adapt**
- **Make it simple for a child to succeed**

---

## üöß Current Status

- [x] Spec ([markdown](./spec/AmysEcho.md))
- [x] React Native baseline setup
- [x] Camera + ML integration (initial hybrid recognizer)
- [x] HIP 1 + HIP 3 MVP implementation
- [x] HIP 2 training flow stub

## Project Status & Open Todos

The project has a stable foundation after a major refactor. The database, navigation, and core app structure are complete. The next steps focus on implementing the scaffolded features to reach the goal defined in `/spec/AmysEcho.md`.

**For detailed implementation instructions, see [`docs/TODO.md`](docs/TODO.md) and [`docs/UnifiedAIImplementationBlueprint.md`](docs/UnifiedAIImplementationBlueprint.md).**

### Priority 1: Activate Core Functionality
- [x] **Implement Gesture Recognition**: The `mlService.ts` now loads the TFLite model and performs live gesture classification.
- [x] **Implement Rich Audio Feedback**: The `audioService.ts` plays success and error sounds using `expo-av`.

### Priority 2: Enhance with Intelligence & Accessibility
- [x] **Integrate Live LLM Dialog Engine**: `dialogEngine.ts` now makes a live OpenAI API request for suggestions.
- [x] **Add DGS Video Playback**: DGS videos can be shown via a toggle on the `LearningScreen`.

### Priority 3: Polish and Administration
- [x] **Complete Admin Panel**: CRUD functionality in `AdminScreen.tsx` manages symbols and vocabularies.
- [x] **UI/UX Polish**: Conducted a full review of the UI, improved layouts, and ensured all buttons and interactive elements have accessibility labels.

### Priority 4: Personalized AI Pipeline
- [x] **Acquire Pre-trained Models**: Download and bundle the MediaPipe models (see `src/tools/downloadModels.ts`).
- [x] **Implement Two-Stage Frame Processor**: Load landmark and gesture models inside a `useFrameProcessor` worklet.
- [x] **Build TrainingScreen UI**: Provide a guided interface for recording gesture samples.
- [x] **Implement In-Memory Landmark Extraction**: Use ffmpeg to pull frames and save landmarks only.
 - [x] **Direct OpenAI Integration**: Suggestions are fetched from OpenAI using an API key stored on the device.
- [x] **Create Model Training Endpoint & Script**: Accept uploaded landmarks and train an LSTM gesture model.
- [x] **Create Model Download Endpoint**: Serve the latest personalized model to the app.
- [x] **Implement Model Download and Activation**: Download the `.tflite` model and store its URI securely.
- [x] **Activate the Personalized Model**: Use the custom model in the Learning screen when available.

### Priority 5: Adaptive Learning & Maintenance
- [x] **Implement Adaptive Learning Service (ALS)**: Dynamically adjust gesture confidence thresholds and health scores after each interaction, triggering HIP 4 when necessary.
- [x] **Show Proactive Maintenance Banner (HIP 4)**: Display a soft banner asking for practice when a gesture's healthScore declines.
- [x] **Sync Training Data with Consent**: Batch upload pending `gesture_training_data` to the server when the caregiver allows it and a Wi-Fi connection is available.
- [x] **Finish LSTM Training Pipeline**: Replace the placeholder logic in `train.py` with a real LSTM model that outputs a `.tflite` file.
- [x] **Automate Offline Model Updates**: Regularly download newly trained personalized models and refresh the local classifier.
- [x] **Log Caregiver Corrections**: Misidentified gestures are saved in the new `corrections` table for future training.
- [x] **Background Sync Service**: Unsynced corrections are uploaded and model updates are checked on app launch.

## ‚ñ∂Ô∏è Running the mobile app

The React Native code lives in `app/`. Install dependencies with `npm install` inside that folder, then run `npm run ios` or `npm run android` to start a simulator. This skeleton includes onboarding, recognition, correction and training screens. Camera and ML integration now have an initial hybrid recognizer stub.

DGS demonstration videos can be placed under `app/assets/videos/dgs/`. Each gesture entry may specify a `videoUri` and optional `dgsVideoUri` pointing to these files. A toggle on the recognition screen lets you switch between the standard symbol video and the DGS version when available.

The LLM-powered suggestions require an OpenAI API key. You can set this via the `OPENAI_API_KEY` environment variable, place the key in a local `.openai-key` file, or save it securely using the Admin screen. Never commit keys to the repository.

### Building the custom dev client

If you want to run the app on a physical device with a custom dev client, execute `npx expo prebuild` inside `app/` once to generate the native projects. Afterwards use `npm run ios` or `npm run android` to install the dev client on your device.

### Creating production builds

To generate store-ready binaries using EAS Build, run:

```bash
npm run build:android
npm run build:ios
```

This uses `eas.json` and requires credentials configured with Expo.

### Retraining the offline model

Collected gesture samples can be used to update the local fallback model. Run:

```bash
npm run build
node dist/tools/retrainOfflineModel.js <path/to/db.json> dist/offlineModel.json
```

The recognizer will load `dist/offlineModel.json` by default when classifying offline.

### Updating analytics

Run the analytics updater to refresh caregiver stats stored in the database:

```bash
npm run build
node dist/tools/updateAnalytics.js <path/to/db.json>
```

### Running the backend server

The backend provides endpoints for model training.
Start it with:

```bash
npm run build
node dist/server.js
```

Set an `API_TOKEN` environment variable before starting.
Requests must include `Authorization: Bearer $API_TOKEN`.

Open the app and tap **Analytics** on the recognition screen to view the dashboard.

---

## ü§ù Contributing

This is a focused project with one user. That means:

- ‚úÖ Clean code, tested assumptions
- ‚úÖ No ‚Äúmove fast‚Äù hacks
- ‚úÖ Emotional context matters ‚Äî build with care

If you‚Äôre here to help: thank you.  
PRs are welcome, but **read the spec first**.

---

## üìÑ License

MIT ‚Äì But with one request:  
**If you use this work to help another child ‚Äî let me know.** That‚Äôs why it‚Äôs public.

---

## ‚ù§Ô∏è Built For

**Amy.**  
To help her be understood.  
To help her learn.  
To help the world finally listen.
